# Anthropic API Key
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# OpenAI API Key (optional, for OpenAI model support)
OPENAI_API_KEY=your_openai_api_key_here

# ============================================================================
# TEXT-TO-SPEECH CONFIGURATION
# ============================================================================
# Choose ONE of the following TTS options:
#   1. ElevenLabs (cloud-based) - Set ELEVENLABS_API_KEY
#   2. XTTS v2 (local) - Set XTTS_ENABLED=true
#   3. StyleTTS 2 (local) - Set STYLETTS2_ENABLED=true
#
# Priority order: StyleTTS 2 > XTTS > ElevenLabs

# ElevenLabs TTS Configuration (optional, cloud-based TTS)
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
# Optional: Custom voice ID (default: Rachel - 21m00Tcm4TlvDq8ikWAM)
# ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM
# Optional: Custom model ID (default: eleven_multilingual_v2)
# ELEVENLABS_MODEL_ID=eleven_multilingual_v2
# Optional: Multiple voices (JSON array) - adds voice selector dropdown in settings
# Format: [{"voice_id": "...", "label": "Name", "description": "Optional description"}]
# Example voices:
ELEVENLABS_VOICES='[
    {"voice_id": "21m00Tcm4TlvDq8ikWAM", "label": "Rachel", "description": "Calm female"},
    {"voice_id": "ErXwobaYiN019PkySvjV", "label": "Antoni", "description": "Warm male"}]'

# XTTS v2 Local TTS Configuration (optional, local TTS with voice cloning)
# Requires the XTTS server to be running. Start it with:
#   1. Install PyTorch first (GPU): pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
#      Or for CPU only: pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
#   2. Install XTTS deps: pip install -r requirements-xtts.txt
#   3. Run the server: python run_xtts.py
# XTTS_ENABLED=true
# XTTS_API_URL=http://localhost:8020
# XTTS_LANGUAGE=en
# XTTS_VOICES_DIR=./xtts_voices
# Optional: Default speaker sample file path (if not using cloned voices)
# XTTS_DEFAULT_SPEAKER=/path/to/speaker.wav
# Optional: Pre-load speaker latents on startup (comma-separated paths)
# Speaker latents are cached to avoid recomputing them for each TTS request.
# If XTTS_VOICES_DIR/voices.json exists, those voices are also preloaded automatically.
# XTTS_PRELOAD_SPEAKERS=/path/to/speaker1.wav,/path/to/speaker2.wav

# StyleTTS 2 Local TTS Configuration (optional, local TTS with voice cloning)
# StyleTTS 2 takes priority over XTTS and ElevenLabs if enabled.
# Requires the StyleTTS 2 server to be running. Start it with:
#   1. Install PyTorch first (GPU): pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
#      Or for CPU only: pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
#   2. Install StyleTTS 2 deps: pip install -r requirements-styletts2.txt
#   3. Run the server: python run_styletts2.py
# STYLETTS2_ENABLED=true
# STYLETTS2_API_URL=http://localhost:8021
# STYLETTS2_VOICES_DIR=./styletts2_voices
# Optional: Default speaker sample file path (if not using cloned voices)
# STYLETTS2_DEFAULT_SPEAKER=/path/to/speaker.wav
# Optional: Pre-load speaker embeddings on startup (comma-separated paths)
# STYLETTS2_PRELOAD_SPEAKERS=/path/to/speaker1.wav,/path/to/speaker2.wav
#
# Phonemizer backend: "gruut" (default, MIT licensed, no system deps) or "espeak"
# gruut is recommended unless you specifically need espeak-ng quality.
# If using espeak, install espeak-ng on your system first.
# STYLETTS2_PHONEMIZER=gruut
#
# Pronunciation fixes: JSON object mapping mispronounced words to phonetic spellings.
# StyleTTS 2 sometimes mispronounces certain words; this fixes them before synthesis.
# Default fixes are used if not set. Set to {} to disable all fixes.
# STYLETTS2_PRONUNCIATION_FIXES='{"turned": "turnd", "learned": "lernd", "burned": "burnd", "earned": "ernd", "into": "in to"}'

# ============================================================================
# SPEECH-TO-TEXT CONFIGURATION
# ============================================================================
# Whisper STT Local Speech-to-Text Configuration (optional)
# Provides high-quality transcription with punctuation (replaces browser dictation)
# Requires the Whisper server to be running. Start it with:
#   1. Install PyTorch first (GPU): pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
#      Or for CPU only: pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu
#   2. Install Whisper deps: pip install -r requirements-whisper.txt
#   3. Run the server: python run_whisper.py
# WHISPER_ENABLED=true
# WHISPER_API_URL=http://localhost:8030
# Available models: tiny, base, small, medium, large-v2, large-v3, distil-large-v3
# WHISPER_MODEL=large-v3
# Dictation mode determines which speech-to-text method the frontend uses:
#   - "whisper": Always use local Whisper (fails if server unavailable)
#   - "browser": Always use browser's Web Speech API (no punctuation)
#   - "auto": Use Whisper if available, fall back to browser (default)
# DICTATION_MODE=auto

# ============================================================================
# TOOL USE CONFIGURATION
# ============================================================================
# Enable AI tool use (web search, content fetching)
# TOOLS_ENABLED=true
# Maximum iterations for tool use before forcing final response (default: 10)
# TOOL_USE_MAX_ITERATIONS=10

# Brave Search API key (for web search tool)
# Get your API key at: https://brave.com/search/api/
BRAVE_SEARCH_API_KEY=your_brave_search_api_key_here

# ============================================================================
# CODEBASE NAVIGATOR CONFIGURATION
# ============================================================================
# Enable cost-efficient codebase exploration using Mistral Devstral model.
# The navigator allows AI entities to analyze large codebases before implementing
# changes, identifying relevant files and understanding code architecture.
#
# Cost comparison (per 500k token codebase scan):
#   - Claude Opus 4.5: $2.50
#   - Devstral 2: $0.025 (100x cheaper)
#
# CODEBASE_NAVIGATOR_ENABLED=true

# Mistral API key (required for navigator)
# Get your API key at: https://console.mistral.ai/
# MISTRAL_API_KEY=your_mistral_api_key_here

# Model to use for navigation (default: devstral-small-latest)
# Options: devstral-small-latest, devstral-medium-latest, devstral-2512
# CODEBASE_NAVIGATOR_MODEL=devstral-small-latest

# API timeout in seconds (default: 120)
# CODEBASE_NAVIGATOR_TIMEOUT=120

# Maximum tokens per codebase chunk (default: 200000)
# Leave headroom in Devstral's 256k context window
# CODEBASE_NAVIGATOR_MAX_TOKENS_PER_CHUNK=200000

# Maximum files to return in results (default: 50)
# CODEBASE_NAVIGATOR_MAX_RESULTS=50

# Cache configuration
# CODEBASE_NAVIGATOR_CACHE_ENABLED=true
# CODEBASE_NAVIGATOR_CACHE_DIR=.navigator_cache
# CODEBASE_NAVIGATOR_CACHE_TTL_HOURS=24

# File patterns (JSON arrays)
# Include patterns: Which files to analyze
# CODEBASE_NAVIGATOR_DEFAULT_INCLUDES='["*.py","*.js","*.ts","*.jsx","*.tsx","*.java","*.go","*.rs","*.c","*.cpp","*.h","*.json","*.yaml","*.yml","*.toml","*.md","*.sql","*.graphql","*.html","*.css","*.scss"]'
# Exclude patterns: Which files/directories to skip
# CODEBASE_NAVIGATOR_DEFAULT_EXCLUDES='["node_modules/","venv/",".venv/","__pycache__/",".git/","dist/","build/",".next/","*.min.js","*.map","*.lock","*.bundle.js"]'

# ============================================================================
# GITHUB INTEGRATION CONFIGURATION
# ============================================================================
# Enable GitHub repository integration for AI entities
# GITHUB_TOOLS_ENABLED=true

# Configure repositories (JSON array)
# Each repository requires:
#   - owner: GitHub username or organization
#   - repo: Repository name
#   - label: Display name for the repository
#   - token: GitHub Personal Access Token (classic or fine-grained)
# Optional fields:
#   - protected_branches: Branches that cannot be committed to (default: ["main", "master"])
#   - capabilities: Allowed operations (default: ["read", "branch", "commit", "pr", "issue"])
#   - local_clone_path: Path to local clone for faster operations (optional)
#       Supports both Unix (/path/to/repo) and Windows (C:\path\to\repo) paths
#   - commit_author_name: Name for commit attribution (optional, defaults to PAT owner)
#   - commit_author_email: Email for commit attribution (optional, defaults to PAT owner)
#
# Token permissions needed:
#   - Read-only: Contents (read)
#   - Full access: Contents (read/write), Pull requests (read/write), Issues (read/write)
#
# Example configuration:
# GITHUB_REPOS='[
#     {
#         "owner": "your-username",
#         "repo": "your-repo",
#         "label": "My Project",
#         "token": "ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx",
#         "protected_branches": ["main", "master", "production"],
#         "capabilities": ["read", "branch", "commit", "pr", "issue"],
#         "commit_author_name": "Your Name",
#         "commit_author_email": "your.email@example.com"
#     },
#     {
#         "owner": "your-org",
#         "repo": "another-repo",
#         "label": "Org Project (Read Only)",
#         "token": "ghp_yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy",
#         "capabilities": ["read"]
#     }
# ]'

# Pinecone Configuration
# Each index represents a different AI entity with separate memory/conversation history
# Format: JSON array of objects with:
#   - index_name: Pinecone index name
#   - label: Display name for the entity
#   - description: Optional description
#   - llm_provider: "anthropic" or "openai" (default: "anthropic")
#   - default_model: Model to use for this entity (optional, uses provider default if not set)
#   - host: url of your Pinecone index's host server
# Example with different providers:
PINECONE_INDEXES='[
    {"index_name": "claude-main", "label": "Claude", "llm_provider": "anthropic", "host": "[Your Pincone index host url]", "default_model": "claude-sonnet-4-5-20250929"},
    {"index_name": "gpt-research", "label": "GPT", "llm_provider": "openai", "host": "[Your Pincone index host url]", "default_model": "GPT-5.1"}
]'

# ============================================================================
# MEMORY RETRIEVAL CONFIGURATION
# ============================================================================
# Role balance ensures retrieved memories include both human and assistant messages
# when possible. This provides more balanced context by preventing memory sets
# that are entirely from one role. Set to false to select purely by relevance score.
# MEMORY_ROLE_BALANCE_ENABLED=true

# Database URL (SQLite for development)
HERE_I_AM_DATABASE_URL=sqlite+aiosqlite:///./here_i_am.db

# Optional: PostgreSQL for production
# HERE_I_AM_DATABASE_URL=postgresql+asyncpg://user:password@localhost/here_i_am

# Application Settings
DEBUG=true
